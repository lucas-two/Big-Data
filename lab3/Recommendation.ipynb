{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basics of Recommendation Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.spatial.distance import correlation, cosine\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 4)\n",
      "[[  4.   3.   2.   3.]\n",
      " [  1.   2.   3.   1.]\n",
      " [ nan   2.   1.  nan]\n",
      " [  4.   3.  nan  nan]]\n"
     ]
    }
   ],
   "source": [
    "#TODO: load dataset into variable M\n",
    "import numpy as np\n",
    "\n",
    "data = [\n",
    "    [4.0,    3.0,   2.0,    3.0],\n",
    "    [1.0,    2.0,   3.0,    1.0],\n",
    "    [np.nan, 2.0,   1.0,    np.nan],\n",
    "    [4.0,    3.0,   np.nan, np.nan]\n",
    "]\n",
    "\n",
    "M = np.array(data)\n",
    "\n",
    "# Display M\n",
    "print(M.shape)\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def cosine_similarity(v1,v2, metric='cosine'):\n",
    "    #metric: cosine or correlation\n",
    "    if metric == 'correlation':\n",
    "        v1 = v1 - np.nanmean(v1)\n",
    "        v2 = v2 - np.nanmean(v2)\n",
    "    \"compute similarity of v1 to v2: (v1 dot v2)/{||v1||*||v2||)\"\n",
    "    sumxx, sumxy, sumyy = 0, 0, 0\n",
    "    for i in range(len(v1)):\n",
    "        x = v1[i]; y = v2[i]\n",
    "        if np.isnan(x) or np.isnan(y): continue\n",
    "        sumxx += x*x\n",
    "        sumyy += y*y\n",
    "        sumxy += x*y\n",
    "    return sumxy/math.sqrt(sumxx*sumyy)\n",
    "\n",
    "def sim_matrix(M, dimension='user', metric='cosine'):\n",
    "    N = M.shape[0] if dimension == 'user' else M.shape[1]\n",
    "    sim = np.zeros([N,N])\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            if i == j:\n",
    "                sim[i,j] = 0 #Cancel out the effect of self-similarity in the sums later\n",
    "                continue\n",
    "            if dimension == 'user':\n",
    "                v1, v2 = M[i,:], M[j,:]\n",
    "            else:\n",
    "                v1, v2 = M[:,i], M[:,j]\n",
    "            sim[i][j] = cosine_similarity(v1,v2,metric)\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99227787671366774"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(M[0,:], M[2,:], 'cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.79582243,  0.99227788,  1.        ],\n",
       "       [ 0.79582243,  0.        ,  0.86824314,  0.89442719],\n",
       "       [ 0.99227788,  0.86824314,  0.        ,  1.        ],\n",
       "       [ 1.        ,  0.89442719,  1.        ,  0.        ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_matrix(M, 'user')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.9649505 ,  0.73994007,  0.99705449],\n",
       "       [ 0.9649505 ,  0.        ,  0.90748521,  0.96476382],\n",
       "       [ 0.73994007,  0.90748521,  0.        ,  0.78935222],\n",
       "       [ 0.99705449,  0.96476382,  0.78935222,  0.        ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_matrix(M, 'item')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70710678118654746"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(M[0,:], M[2,:], 'correlation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -0.85280287,  0.70710678,  0.70710678],\n",
       "       [-0.85280287,  0.        , -0.5547002 , -0.89442719],\n",
       "       [ 0.70710678, -0.5547002 ,  0.        , -1.        ],\n",
       "       [ 0.70710678, -0.89442719, -1.        ,  0.        ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_matrix(M, 'user', 'correlation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.94280904, -0.89442719,  0.9486833 ],\n",
       "       [ 0.94280904,  0.        ,  0.        ,  1.        ],\n",
       "       [-0.89442719,  0.        ,  0.        , -0.70710678],\n",
       "       [ 0.9486833 ,  1.        , -0.70710678,  0.        ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_matrix(M, 'item', 'correlation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Compute the missing rating in this table using user-based collaborative filtering (CF). (Use cosine similarity, then use Pearson similarity). Assume taking all neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def user_cf(M, metric='cosine'):\n",
    "    pred = np.copy(M)\n",
    "    n_users, n_items = M.shape\n",
    "    avg_ratings = np.nanmean(M, axis=1)\n",
    "    sim_users = sim_matrix(M, 'user', metric)\n",
    "    for i in range(n_users):\n",
    "        for j in range(n_items):\n",
    "            if np.isnan(M[i,j]):\n",
    "                # We are to use the actual formula for this as in the lecture slides\n",
    "                pred[i,j] = avg_ratings[i] + np.nansum(sim_users[i] * (M[:,j] - avg_ratings)) / sum(sim_users[i])                \n",
    "                                                    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of this part of the code:  \n",
    "<code>np.nansum(sim_users[i] * (M[:,j] - avg_ratings))</code>  \n",
    "\n",
    "if j = 1:  \n",
    "sim_users[i] x (M[0,1] - avg_ratings)  \n",
    "plus    \n",
    "sim_users[i] x (M[1,1] - avg_ratings)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-based CF (Cosine): \n",
      "          0    1         2         3\n",
      "0  4.000000  3.0  2.000000  3.000000\n",
      "1  1.000000  2.0  3.000000  1.000000\n",
      "2  1.794036  2.0  1.000000  1.272355\n",
      "3  4.000000  3.0  3.368034  3.268237\n",
      "User-based CF (Pearson): \n",
      "          0    1         2         3\n",
      "0  4.000000  3.0  2.000000  3.000000\n",
      "1  1.000000  2.0  3.000000  1.000000\n",
      "2  0.764822  2.0  1.000000  1.009169\n",
      "3  4.000000  3.0  4.616077  2.935013\n"
     ]
    }
   ],
   "source": [
    "print(\"User-based CF (Cosine): \\n\" + str(pd.DataFrame(user_cf(M, 'cosine'))))\n",
    "print(\"User-based CF (Pearson): \\n\" + str(pd.DataFrame(user_cf(M, 'correlation'))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Similarly, computing the missing rating using item-based CF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def item_cf(M, metric='cosine'):\n",
    "    pred = np.copy(M)\n",
    "    n_users, n_items = M.shape\n",
    "    avg_ratings = np.nanmean(M, axis=0)\n",
    "    sim_items = sim_matrix(M, 'item', metric)\n",
    "    for i in range(n_users):\n",
    "        for j in range(n_items):\n",
    "            if np.isnan(M[i,j]):\n",
    "                # very similar to the previous, just using sim_items now instead\n",
    "                pred[i,j] = avg_ratings[i] + np.nansum(sim_items[i] * (M[:,j] - avg_ratings)) / sum(sim_items[i])\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item-based CF (Cosine): \n",
      "          0    1         2         3\n",
      "0  4.000000  3.0  2.000000  3.000000\n",
      "1  1.000000  2.0  3.000000  1.000000\n",
      "2  2.392903  2.0  1.000000  1.441382\n",
      "3  4.000000  3.0  1.526011  1.473989\n",
      "Item-based CF (Pearson): \n",
      "          0    1         2         3\n",
      "0  4.000000  3.0  2.000000  3.000000\n",
      "1  1.000000  2.0  3.000000  1.000000\n",
      "2  3.441518  2.0  1.000000  2.000000\n",
      "3  4.000000  3.0  2.208141  0.791859\n"
     ]
    }
   ],
   "source": [
    "print(\"Item-based CF (Cosine): \\n\" + str(pd.DataFrame(item_cf(M, 'cosine'))))\n",
    "print(\"Item-based CF (Pearson): \\n\" + str(pd.DataFrame(item_cf(M, 'correlation'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluating Recommendation Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictive Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3\n",
       "0  4  3  2  3\n",
       "1  1  2  3  1\n",
       "2  1  2  1  2\n",
       "3  4  3  2  4"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M_result = np.asarray([[4,3,2,3], \n",
    "                [1,2,3,1],\n",
    "                [1,2,1,2],\n",
    "                [4,3,2,4]])\n",
    "pd.DataFrame(M_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluateRS(ratings, groundtruth, method='user_cf', metric='cosine'):\n",
    "    #method: user_cf and item_cf, metric: cosine and correlation\n",
    "    if method == 'user_cf':\n",
    "        prediction = user_cf(ratings, metric)\n",
    "    else:\n",
    "        prediction = item_cf(ratings, metric)\n",
    "    MSE = mean_squared_error(prediction, groundtruth)\n",
    "    RMSE = round(math.sqrt(MSE),3)\n",
    "    print(\"RMSE using {0} approach ({2}) is: {1}\".format(method, RMSE, metric))\n",
    "    # print(pd.DataFrame(prediction))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE using user_cf approach (cosine) is: 0.472\n",
      "RMSE using user_cf approach (correlation) is: 0.751\n",
      "RMSE using item_cf approach (cosine) is: 0.744\n",
      "RMSE using item_cf approach (correlation) is: 1.009\n"
     ]
    }
   ],
   "source": [
    "#TODO: evaluate the predictive accuracy\n",
    "evaluateRS(M, M_result, 'user_cf', 'cosine')\n",
    "evaluateRS(M, M_result, 'user_cf', 'correlation')\n",
    "evaluateRS(M, M_result, 'item_cf', 'cosine')\n",
    "evaluateRS(M, M_result, 'item_cf', 'correlation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "def evaluate_rank(ratings, groundtruth, method='user_cf', metric='cosine'):\n",
    "    #metric: cosine vs correlation\n",
    "    if method == 'user_cf':\n",
    "        prediction = user_cf(ratings, metric)\n",
    "    else:\n",
    "        prediction = item_cf(ratings, metric)\n",
    "    \n",
    "    avg_tau = 0\n",
    "    n_users, n_items = M.shape # need to declare what n_users is\n",
    "    for i in range(n_users):\n",
    "        tau, p_value = stats.kendalltau(M_result[i,:], prediction[i,:])\n",
    "        avg_tau += tau\n",
    "    avg_tau = avg_tau / n_users\n",
    "    #clear_output(wait=True)\n",
    "    print(\"Rank accuracy using {0} approach ({2}) is: {1}\".format(method, avg_tau, metric))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank accuracy using user_cf approach (cosine) is: 0.6477056190747297\n",
      "Rank accuracy using user_cf approach (correlation) is: 0.56719350585564\n",
      "Rank accuracy using item_cf approach (cosine) is: 0.5456435464587639\n",
      "Rank accuracy using item_cf approach (correlation) is: 0.5456435464587639\n"
     ]
    }
   ],
   "source": [
    "#TODO: calculate the ranking accuracy\n",
    "evaluate_rank(M, M_result, 'user_cf', 'cosine')\n",
    "evaluate_rank(M, M_result, 'user_cf', 'correlation')\n",
    "evaluate_rank(M, M_result, 'item_cf', 'cosine')\n",
    "evaluate_rank(M, M_result, 'item_cf', 'correlation')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
